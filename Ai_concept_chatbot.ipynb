{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssefelsayed148/AI_tutor_chatbot/blob/main/Ai_concept_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and model"
      ],
      "metadata": {
        "id": "h4t1uhIY2nT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e08VyWIEP-LK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUwE19rqQ_Yv"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your token inside the string\n",
        "login(token=\"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcOKajhpP_p_"
      },
      "outputs": [],
      "source": [
        "model_id =   \"google/gemma-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype='auto',\n",
        ")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt function"
      ],
      "metadata": {
        "id": "2YGYjtiP2j9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQp99yGlQQys"
      },
      "outputs": [],
      "source": [
        "def build_gemma_prompt(concept_text):\n",
        "    return f\"\"\"You are an expert AI tutor and note-taking assistant. Your job is to help users understand machine learning and AI concepts.\n",
        "\n",
        "[EXAMPLE]\n",
        "\n",
        "Text: Overfitting is a problem in machine learning where a model performs well on training data but poorly on new data. It happens when the model learns noise or irrelevant patterns instead of the true signal. Overfitting is often caused by having too many parameters or not enough data. It leads to poor generalization. Techniques like regularization or cross-validation are used to reduce overfitting.\n",
        "\n",
        "Summary:\n",
        "Overfitting happens when a machine learning model learns the training data too well, including noise. This results in poor performance on new or unseen data. It’s a sign that the model hasn’t generalized well. Overfitting is common with complex models or small datasets. Regularization and cross-validation are techniques used to avoid it.\n",
        "\n",
        "Key Terms:\n",
        "1. Overfitting\n",
        "2. Generalization\n",
        "3. Noise\n",
        "4. Regularization\n",
        "5. Cross-validation\n",
        "\n",
        "Flashcards:\n",
        "Q1: What is overfitting in machine learning?\n",
        "A1: When a model performs well on training data but poorly on new data because it learned noise.\n",
        "\n",
        "Q2: Why does overfitting happen?\n",
        "A2: It can occur due to complex models, too many parameters, or not enough data.\n",
        "\n",
        "Q3: What does overfitting indicate about generalization?\n",
        "A3: That the model has poor generalization to new data.\n",
        "\n",
        "Q4: Name two methods to reduce overfitting.\n",
        "A4: Regularization and cross-validation.\n",
        "\n",
        "Q5: How is overfitting different from underfitting?\n",
        "A5: Overfitting learns too much (including noise); underfitting learns too little.\n",
        "\n",
        "[NEW INPUT]\n",
        "\n",
        "Text: {concept_text.strip()}\n",
        "\n",
        "Now follow the same format with a new response.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbSMyMmwbLXd",
        "outputId": "71136c75-2eb3-40ef-9f86-9d507e57ba8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here is the response you requested:\n",
            "\n",
            "**Q1: What is gradient descent in machine learning?**\n",
            "A1: Gradient descent is an optimization algorithm used to minimize a loss function by updating model parameters in the direction of the negative gradient.\n",
            "\n",
            "**Q2: What is the purpose of gradient descent?**\n",
            "A2: The purpose of gradient descent is to find the set of model parameters that minimizes the loss function. This is achieved by iteratively updating the parameters in the direction of the negative gradient, which is the direction that leads to the lowest loss value.\n",
            "\n",
            "**Q3: How does gradient descent work?**\n",
            "A3: Gradient descent works by iteratively updating the model parameters in the direction of the negative gradient. The loss function is a measure of how well the model fits the training data. The negative gradient is the direction that points to the direction of the steepest descent of the loss function. Gradient descent updates the model parameters in the direction of the negative gradient, which is the direction that leads to the lowest loss value.\n",
            "\n",
            "**Q4: What are the different types of gradient descent?**\n",
            "A4: There are two main types of gradient descent: batch gradient descent and stochastic gradient descent.\n",
            "\n",
            "**Q5: How is gradient descent different from other optimization algorithms?**\n",
            "A5: Gradient descent is an iterative optimization algorithm, meaning that it updates the model parameters in a loop. Other optimization algorithms, such as linear search, can find the minimum of a function by iterating over all possible parameter values and finding the one that minimizes the function.\n"
          ]
        }
      ],
      "source": [
        "concept = \"\"\"Gradient descent is an optimization algorithm used to minimize a loss function by updating\n",
        "model parameters in the direction of the negative gradient...\"\"\"\n",
        "\n",
        "prompt = build_gemma_prompt(concept)\n",
        "output = pipe(prompt, max_new_tokens=700, temperature=0.3, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"].replace(prompt, \"\").strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the prompt function"
      ],
      "metadata": {
        "id": "vhgnfxwY2f9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaBUdFJWbT9q"
      },
      "outputs": [],
      "source": [
        "def build_gemma_prompt(concept_text):\n",
        "    return f\"\"\"You are an expert AI tutor and note-taking assistant. Your job is to help users understand machine learning and AI concepts.\n",
        "\n",
        "[EXAMPLE]\n",
        "\n",
        "Text: Overfitting is a problem in machine learning where a model performs well on training data but poorly on new data. It happens when the model learns noise or irrelevant patterns instead of the true signal. Overfitting is often caused by having too many parameters or not enough data. It leads to poor generalization. Techniques like regularization or cross-validation are used to reduce overfitting.\n",
        "\n",
        "Summary:\n",
        "Overfitting happens when a machine learning model learns the training data too well, including noise. This results in poor performance on new or unseen data. It’s a sign that the model hasn’t generalized well. Overfitting is common with complex models or small datasets. Regularization and cross-validation are techniques used to avoid it.\n",
        "\n",
        "Key Terms:\n",
        "1. Overfitting\n",
        "2. Generalization\n",
        "3. Noise\n",
        "4. Regularization\n",
        "5. Cross-validation\n",
        "\n",
        "Flashcards:\n",
        "Q1: What is overfitting in machine learning?\n",
        "A1: When a model performs well on training data but poorly on new data because it learned noise.\n",
        "\n",
        "Q2: Why does overfitting happen?\n",
        "A2: It can occur due to complex models, too many parameters, or not enough data.\n",
        "\n",
        "Q3: What does overfitting indicate about generalization?\n",
        "A3: That the model has poor generalization to new data.\n",
        "\n",
        "Q4: Name two methods to reduce overfitting.\n",
        "A4: Regularization and cross-validation.\n",
        "\n",
        "Q5: How is overfitting different from underfitting?\n",
        "A5: Overfitting learns too much (including noise); underfitting learns too little.\n",
        "\n",
        "[NEW INPUT]\n",
        "\n",
        "Text: {concept_text.strip()}\n",
        "\n",
        "Now follow the same format and return:\n",
        "1. A 5-sentence summary\n",
        "2. 5 key terms\n",
        "3. 5 flashcards\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzsAjEondMNy",
        "outputId": "e7a8acec-5f0c-4831-ce9b-838dbc389cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Summary:**\n",
            "The provided text explains the concept of gradient descent and its role in minimizing a loss function.\n",
            "\n",
            "**Key Terms:**\n",
            "1. Gradient descent\n",
            "2. Loss function\n",
            "3. Learning rate\n",
            "4. Mini-batch gradient descent\n",
            "5. Stochastic gradient descent\n",
            "\n",
            "**Flashcards:**\n",
            "Q1: What is the purpose of gradient descent?\n",
            "A1: To minimize a loss function by updating model parameters in the direction of the negative gradient.\n",
            "\n",
            "Q2: How does the learning rate affect gradient descent?\n",
            "A2: It controls the step size, which determines the size of the update.\n",
            "\n",
            "Q3: What is the difference between gradient descent and stochastic gradient descent?\n",
            "A3: Gradient descent updates all parameters in each iteration, while stochastic gradient descent updates only a subset of parameters.\n",
            "\n",
            "Q4: What is the purpose of mini-batch gradient descent?\n",
            "A4: It reduces the computational cost by using a subset of data for each update.\n",
            "\n",
            "Q5: What is the difference between gradient descent and stochastic gradient descent?\n",
            "A5: Gradient descent updates all parameters in each iteration, while stochastic gradient descent updates only a subset of parameters.\n"
          ]
        }
      ],
      "source": [
        "concept = \"\"\"Gradient descent is an optimization algorithm used to minimize a loss function\n",
        "by updating model parameters in the direction of the negative gradient. The learning rate controls the step size.\n",
        "It is widely used in training machine learning models like linear regression and neural networks.\n",
        "Variants include stochastic and mini-batch gradient descent. Proper tuning of the learning rate is critical to convergence.\"\"\"\n",
        "\n",
        "prompt = build_gemma_prompt(concept)\n",
        "output = pipe(prompt, max_new_tokens=1500, temperature=0.3, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"].replace(prompt, \"\").strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XlI0GvgdYQD",
        "outputId": "9a2d4599-7a41-4839-873d-a02622b6743e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**1. Summary**\n",
            "The transformer architecture is a deep learning model designed for sequence data. It uses self-attention mechanisms instead of recurrence, enabling parallel processing. Transformers excel in tasks like language modeling, translation, and summarization.\n",
            "\n",
            "**2. Key Terms**\n",
            "1. Transformer architecture\n",
            "2. Self-attention mechanisms\n",
            "3. Encoder\n",
            "4. Decoder\n",
            "5. Sequence data\n",
            "\n",
            "**3. Flashcards**\n",
            "Q1: What is the transformer architecture?\n",
            "A1: A deep learning model designed for sequence data that uses self-attention mechanisms instead of recurrence.\n",
            "\n",
            "Q2: What are self-attention mechanisms?\n",
            "A2: A mechanism that allows the model to attend to different parts of the input sequence at the same time.\n",
            "\n",
            "Q3: What is the purpose of the encoder and decoder blocks?\n",
            "A3: The encoder block processes the input sequence and the decoder block processes the output sequence.\n",
            "\n",
            "Q4: What are the encoder and decoder blocks?\n",
            "A4: The encoder block is responsible for extracting the semantic meaning of the input sequence. The decoder block is responsible for generating the output sequence based on the extracted meaning.\n",
            "\n",
            "Q5: How is the transformer architecture different from recurrent architectures?\n",
            "A5: The transformer architecture is a sequence-to-sequence architecture, while recurrent architectures are used for sequence-to-sequence tasks.\n"
          ]
        }
      ],
      "source": [
        "concept = \"\"\"The transformer architecture is a deep learning model designed for sequence data. \\\n",
        " It relies on self-attention mechanisms instead of recurrence, enabling parallel processing.\\\n",
        " Transformers excel in tasks like language modeling, translation, and summarization. \\\n",
        " The architecture includes encoder and decoder blocks. Popular models like BERT, GPT, and T5 are based on transformers.\n",
        "\"\"\"\n",
        "\n",
        "prompt = build_gemma_prompt(concept)\n",
        "output = pipe(prompt, max_new_tokens=1500, temperature=0.3, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"].replace(prompt, \"\").strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b7jGX7AeWSn",
        "outputId": "f0c57c92-605f-4849-ec7b-6e125ace338c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**1. Summary**\n",
            "Few-shot learning allows models to generalize from a few labeled examples, while zero-shot learning enables inference without any task-specific examples. These capabilities are achieved using large language models, which have demonstrated strong performance in few-shot and zero-shot learning tasks.\n",
            "\n",
            "**2. Key Terms**\n",
            "- Few-shot learning\n",
            "- Zero-shot learning\n",
            "- Prompt engineering\n",
            "\n",
            "**3. Flashcards**\n",
            "Q1: What is the difference between few-shot and zero-shot learning?\n",
            "A1: Few-shot learning requires labeled data for a few tasks, while zero-shot learning requires no labeled data.\n",
            "\n",
            "Q2: What is the role of prompt engineering in few-shot learning?\n",
            "A2: It involves crafting specific prompts to guide the model towards the desired output.\n",
            "\n",
            "Q3: How does prompt engineering impact the model's performance?\n",
            "A3: It can significantly influence the model's ability to generate the desired output.\n",
            "\n",
            "Q4: What is the purpose of prompt engineering in zero-shot learning?\n",
            "A4: It enables the model to generalize to unseen tasks without requiring any labeled data.\n",
            "\n",
            "Q5: How does prompt engineering differ from data augmentation in few-shot learning?\n",
            "A5: While data augmentation involves generating new data points from existing data, prompt engineering focuses on crafting specific prompts to guide the model towards the desired output.\n"
          ]
        }
      ],
      "source": [
        "concept = \"\"\"Few-shot learning allows models to generalize from only a few labeled examples, while zero-shot learning enables inference without any task-specific examples. \\\n",
        "These capabilities are often achieved using large language models.\\\n",
        " Prompt engineering plays a key role in this process.\\\n",
        "  It’s useful for scenarios where labeled data is scarce or unavailable.\\\n",
        "   GPT-style models have demonstrated strong few-shot and zero-shot performance.\n",
        "\"\"\"\n",
        "\n",
        "prompt = build_gemma_prompt(concept)\n",
        "output = pipe(prompt, max_new_tokens=1500, temperature=0.3, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"].replace(prompt, \"\").strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4L4q5Huelgy",
        "outputId": "9715b554-a829-4daf-9a55-52555614d73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**1. Summary**\n",
            "RAG combines retrieval and generation by using an external knowledge base to enhance the model's responses. It addresses hallucination and provides more grounded answers by integrating search with language generation.\n",
            "\n",
            "**2. Key Terms**\n",
            "1. Retrieval\n",
            "2. Generation\n",
            "3. External knowledge base\n",
            "4. Hallucination\n",
            "5. Grounded answers\n",
            "\n",
            "**3. Flashcards**\n",
            "Q1: What is the main purpose of RAG?\n",
            "A1: To reduce hallucination and provide more grounded answers by integrating search with language generation.\n",
            "\n",
            "Q2: What is the role of the external knowledge base in RAG?\n",
            "A2: It provides relevant documents for retrieval and generation.\n",
            "\n",
            "Q3: How does RAG address hallucination?\n",
            "A3: By finding relevant documents from the knowledge base.\n",
            "\n",
            "Q4: What is the main benefit of using RAG?\n",
            "A4: It reduces hallucination and provides more grounded answers.\n",
            "\n",
            "Q5: How does RAG integrate search with language generation?\n",
            "A5: It integrates search to find relevant documents and uses them to generate a response.\n"
          ]
        }
      ],
      "source": [
        "concept = \"\"\"RAG combines retrieval and generation by using an external knowledge base to supplement the model's answers. \\\n",
        "A retriever finds relevant documents, and a generator uses them to create a response.\\\n",
        " It helps reduce hallucination and provides more grounded answers.\\\n",
        "  RAG is commonly used in question-answering and chatbots. \\\n",
        "  It integrates search with language generation.\n",
        "\"\"\"\n",
        "\n",
        "prompt = build_gemma_prompt(concept)\n",
        "output = pipe(prompt, max_new_tokens=1500, temperature=0.3, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"].replace(prompt, \"\").strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWfrOlbgetei",
        "outputId": "f12867f3-2df2-461a-a071-4ef98d39fe4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**1. Summary**\n",
            "Fine-tuning is the process of continuing training a pre-trained language model on a domain-specific dataset. It adapts the model to new tasks or improves performance in a specific context.\n",
            "\n",
            "**2. Key Terms**\n",
            "- Fine-tuning\n",
            "- Pre-trained language model\n",
            "- Domain-specific dataset\n",
            "- Custom AI assistant\n",
            "- Performance\n",
            "\n",
            "**3. Flashcards**\n",
            "Q1: What is fine-tuning?\n",
            "A1: Fine-tuning is the process of continuing training a pre-trained language model on a domain-specific dataset.\n",
            "\n",
            "Q2: What is a pre-trained language model?\n",
            "A2: A language model that has been trained on a massive dataset and is ready to be fine-tuned on a specific task.\n",
            "\n",
            "Q3: What is a domain-specific dataset?\n",
            "A3: A dataset that is tailored to a specific domain, such as a specific genre of text or a specific topic.\n",
            "\n",
            "Q4: What is the purpose of fine-tuning?\n",
            "A4: Fine-tuning allows the model to adapt to a new task or improve its performance in a specific context.\n",
            "\n",
            "Q5: How is fine-tuning different from overfitting?\n",
            "A5: Fine-tuning learns too much (including noise), while overfitting learns too little.\n"
          ]
        }
      ],
      "source": [
        "concept = \"\"\"Fine-tuning is the process of continuing training a pretrained language model on a domain-specific dataset.\\\n",
        " It adapts the model to new tasks or improves performance in a specific context. \\\n",
        " Techniques include full fine-tuning, LoRA, and PEFT.\\\n",
        "  Fine-tuning requires careful data preparation and tuning of hyperparameters. \\\n",
        "  It’s commonly used to build custom AI assistants or specialized models.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "prompt = build_gemma_prompt(concept)\n",
        "output = pipe(prompt, max_new_tokens=1500, temperature=0.3, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"].replace(prompt, \"\").strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the app"
      ],
      "metadata": {
        "id": "mpW8rt0f2aI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_from_ai(prompt):\n",
        "    outputs = pipe(prompt, max_length=512, do_sample=True, temperature=0.7)\n",
        "    return outputs[0][\"generated_text\"].replace(prompt,\" \").strip()\n"
      ],
      "metadata": {
        "id": "17z0j1Pw2IcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tutor_chat(user_input):\n",
        "    prompt = build_gemma_prompt(user_input)\n",
        "    response = get_completion_from_ai(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "d3TzBm-P09fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "kGqYQAJW19xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🤖 AI Tutor Chatbot\")\n",
        "    user_input = gr.Textbox(label=\"Enter you article\")\n",
        "    output = gr.Textbox(label=\"AI Tutor's Response\", lines=20)\n",
        "    user_input.submit(tutor_chat, inputs=user_input, outputs=output)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "SUFdYsJD1rsg",
        "outputId": "baccc264-b2ca-4bb1-82cb-bf930e52235c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0c8f648b70dd12d258.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0c8f648b70dd12d258.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeCI6R912Ehv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}